## 神经网络的表示

左侧**输入层**，中间数值无法直接取得的层级称为**隐藏层**，右侧**输出层**。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114202110038.png" alt="image-20230114202110038" style="zoom:33%;" />

在层级数量的计算上是不考虑输入层的，如上图是一个二层的神经网络。

另外在隐藏层以及输出层上都包含[w,b]的权重参数，其维度要视网络情况判断。例如上图网络的隐藏层的w维度应该是[4,3],包含四个神经元三个输入。



## 神经网络输出计算

下图是神经网络中（以逻辑回归为例），神经元包含两个部分，左侧是权重等参数的计算，右侧是引入激活函数（此处sigmoid）。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114204241852.png" alt="image-20230114204241852" style="zoom:50%;" />

基于此，隐藏层的计算式可以轻易列出：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114204617107.png" alt="image-20230114204617107" style="zoom:33%;" />

然后进行向量化，把所有式子放进矩阵里。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114205201024.png" alt="image-20230114205201024" style="zoom:50%;" />

这样就能够一次性进行整个数据集的计算。



## 激活函数

**已经提到过的sigmoid：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114215020739.png" alt="image-20230114215020739" style="zoom: 50%;" />

**tanh激活函数：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114215055652.png" alt="image-20230114215055652" style="zoom:50%;" />

**RELU激活函数：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114215200902.png" alt="image-20230114215200902" style="zoom:50%;" />

==关于激活函数选择以及特性的问题：==

**选择激活函数的经验论：**

> sigmoid在二分问题才会考虑使用，大部分情况都不会使用。
>
> tanh具有更优越于sigmoid的效果
>
> RELU是绝大多数情况的默认激活函数，原因在于多数输入下导数值都是较大于零的数，学习效率更高。

但是sigmoid以及tanh这两激活函数都存在一个缺点：

> 当输入值远离零点（过大或者过小），函数的导数非常小。（我的印象中这个问题称之为梯度消失），这也会影响在梯度下降法使用的效果。

关于使用非线性激活函数的理解：

> 采取线性激活函数的话，无论有多大的网络，其计算都是线性组合，庞大我网络也就没有意义。

## 神经网络梯度下降法

正向传递得出loss，反向传播梯度来更新权重。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230115172349077.png" alt="image-20230115172349077" style="zoom:50%;" />

具体是怎么实现的呢：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/f529251aa7d3373f368f18d851a016b.jpg" alt="f529251aa7d3373f368f18d851a016b" style="zoom: 25%;" />

这里随意构建了一个神经网络，左侧是正向传递数据得到Loss。右侧则是通过反方向传递梯度来更新权重参数。

> 不难理解，梯度的反向传递实际的原理是链式法则，本身整个神经网络的传递就是函数关系。
>
> 还有一种理解，loss这一误差借助梯度的反向传递也分配到了各个神经元上，每个神经元都存在一定的误差值，梯度下降法的迭代就能够降低误差

如果loss存在理想的最小值，那么在迭代的过程中，梯度会越来越小，权重参数最后没什么变化了，迭代也就完成。



## 随机初始化

在神经网络中需要初始化一个参数，以此为起点进行迭代获得最佳的参数。

> 不能让初始化的值为零，各个[w,b]取零正向传递无法进行，结果也没有意义。

经验论：

>通常考虑是把初始值设为一个较小的数（靠近零），一般是0.01，原因不难理解：例如tanh激活函数，在零附近有着较大的梯度，可以让迭代过程更快的进行。













