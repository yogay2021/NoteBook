## AlexNet主体

> 对于AlexNet的理解认识：
>
> 相较于LeNet，有着更深的网络深度，基本的组合还是若干组【卷积+激活函数+池化】或者是【卷积+激活函数】，接近网络末端时，展开进入全连接层，增加了dropout，随机舍去部分神经元，这样能够减少神经元之间的相关性，避免过拟合。

```python
network_AlexNet = nn.Sequential(
    nn.Conv2D(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=5),
    nn.ReLU(),
    nn.MaxPool2D(kernel_size=2, stride=2),
    nn.Conv2D(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),
    nn.ReLU(),
    nn.MaxPool2D(kernel_size=2, stride=2),
    nn.Conv2D(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),
    nn.ReLU(),
    nn.Conv2D(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),
    nn.ReLU(),
    nn.Conv2D(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2D(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(in_features=256, out_features=4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(in_features=4096, out_features=4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(in_features=4096, out_features=10)
)
```

网络结构的搭建主要还是对于paddle框架高层API的调用，API的具体含义已经在LeNet的笔记中做了详细记录，此处不再重复说明。



## 项目实现过程记录：

> 导入数据集

![image-20230119210422424](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230119210422424.png)

> ALexNet网络主体搭建

![image-20230119210556098](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230119210556098.png)

> 模型封装、参数配置以及模型训练

![image-20230119210826514](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230119210826514.png)

> 模型评估以及推理

![image-20230119211004509](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230119211004509.png)



## 完整的代码：

```python
import paddle
import numpy as np
import matplotlib.pyplot as plt
import paddle.nn as nn

# 数据预处理
import paddle.vision.transforms as T
transform = T.Normalize(mean=[127.5], std=[127.5])
# 训练数据集
train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)
# 验证数据集
eval_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)
print('训练样本量：{}，测试样本量：{}'.format(len(train_dataset), len(eval_dataset)))

#AlexNet网络结构的搭建
network_AlexNet = nn.Sequential(
    nn.Conv2D(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=5),
    nn.ReLU(),
    nn.MaxPool2D(kernel_size=2, stride=2),
    nn.Conv2D(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),
    nn.ReLU(),
    nn.MaxPool2D(kernel_size=2, stride=2),
    nn.Conv2D(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),
    nn.ReLU(),
    nn.Conv2D(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),
    nn.ReLU(),
    nn.Conv2D(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),
    nn.ReLU(),
    nn.MaxPool2D(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(in_features=256, out_features=4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(in_features=4096, out_features=4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(in_features=4096, out_features=10)
)
paddle.summary(network_AlexNet, (1, 1, 28, 28))#可视化模型

model = paddle.Model(network_AlexNet) #模型封装

model.prepare(paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()), 
              paddle.nn.CrossEntropyLoss(), 
              paddle.metric.Accuracy())  #模型配置

model.fit(train_dataset,  
          eval_dataset,   
          epochs=5,       
          batch_size=64,  
          verbose=1)   #模型训练

result = model.evaluate(eval_dataset, verbose=1) #模型评估
print(result)

result = model.predict(eval_dataset) #推理
```

