# Learning Rate Decay



## piecewise decay

分段常数衰减， 在训练过程中不同阶段设置不同的学习率，便于更精细的调参。

boundaries：学习率衰减边界；values：不同阶段对应学习率。



## exponential decay

指数衰减：学习率以指数的形式进行衰减，指数衰减适用于训练过程中需要快速收敛的场景，例如训练较深的网络或者使用大型数据集
$$
learningrate = learningrate_0 * decayrate^{(step/decaysteps)}
$$


其中指数函数的底为decay_rate， 指数为 global_step / decay_steps

 learning_rate: 基学习率；decay_rate: 衰减率；decay_steps: 衰减步数（周期）

staircase: 是否以离散的时间间隔衰减学习率



## natural exponential decay

自然指数衰减:学习率以自然指数进行衰减，其中指数函数底为自然常数e, 指数为-decay_rate * global_step / decay_step， 相比指数衰减具有更快的衰减速度。

learning_rate: 基学习率；decay_rate: 衰减率；decay_steps: 衰减步数/周期；staircase: 是否以离散的时间间隔衰减学习率



## polynomial decay

调整学习率的衰减轨迹以多项式对应的轨迹进行。其中（1 - global_step / decay_steps） 为幂函数的底； power为指数，控制衰减的轨迹。

learning_rate: 基学习率；decay_steps: 衰减率衰减步数；power: 多项式的幂；end_learning_rate：最小学习率



## cosine decay

学习率以cosine 函数曲线进行进行衰减
$$
learningrate = learningrate0 * (1 + cos(pi * epoch / Tmax)) / 2
$$
learning_rate: 基学习率；decay_steps: 衰减率衰减步数；alpha: 最小学习率



## learning rate decay with  momentum

学习率衰减加动量（learning rate decay with  momentum）：与常数衰减类似，但是在每次更新权重时，会加入一个动量项来保持方向性和加速收敛，公式为：v = momentum * v -  learning_rate * gradient；w = w +  v，其中v是动量向量，momentum是动量系数，gradient是梯度，w是权重。学习率衰减加动量适用于训练时需要考虑动量的模型，例如优化循环神经网络或卷积神经网络。



# 一些使用考虑因素





