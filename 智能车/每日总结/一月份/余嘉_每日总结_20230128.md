# 日志

## 早上

1.完成了有监督的数据增强部分，颜色、亮度、对比度随机的代码实现，运行调参观察效果。

2.学习无监督数据增强，阅读资料，了解生成对抗网络GAN 的基本原理。

## 下午：

阅读生成对抗网络的代码，调试GAN代码，运用GAN生成手写数字数据

## 晚上：

1.继续调试GAN，成功生成手写数字数据

2.部署本地训练环境，搭建虚拟环境。

# 学习总结

#### 颜色扰动

##### 颜色随机



<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128101842200-1674915705101-13.png" alt="image-20230128101842200" style="zoom:33%;" />

```python
#图像颜色平衡的随机变化
import cv2
import random
import PIL
import numpy as np
img2 =  PIL.Image.open('E:/DeskTop/photo/dog1.jpg')
rand_po = np.random.randint(0, 100)/10
img_out = ImageEnhance.Color(img2).enhance(rand_po)
img_out.show()
```

> 调用PIL库来完成颜色随机的效果，简单记录API含义。
>
> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128102821982-1674915705102-15.png" alt="image-20230128102821982" style="zoom:50%;" />

> 在编写代码中解决了一个错误，做一个简单的记录，问题如下，发现在改变图像颜色平衡的时候，出现报错，由于调用的是PIL的API，我判断是导入图像格式有问题，问题也确实如此。由cv和PIL导入图像的格式不一样，需要留意。
>
> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128102149329-1674915705102-17.png" alt="image-20230128102149329" style="zoom:67%;" />

> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128102052364-1674915705102-19.png" alt="image-20230128102052364" style="zoom: 33%;" />

##### 亮度随机

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128103153169-1674915705102-21.png" alt="image-20230128103153169" style="zoom:50%;" />

```python
#图像亮度的随机变化
import cv2
import random
import PIL
import numpy as np
img3 =  PIL.Image.open('E:/DeskTop/photo/dog1.jpg')
random_factor = np.random.randint(4, 20) / 10. 
image = ImageEnhance.Brightness(img3).enhance(random_factor)
image.show()
```

> 调用PIL图像亮度变换API，增强系数小于1变暗，大于1 变亮
>
> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128103604470-1674915705102-23.png" alt="image-20230128103604470" style="zoom:67%;" />



##### 对比度随机

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128104012514-1674915705102-27.png" alt="image-20230128104012514" style="zoom:50%;" />

```python
#图像对比度的随机变化
import cv2
import random
import PIL
import numpy as np
img4 =  PIL.Image.open('E:/DeskTop/photo/dog1.jpg')
random_factor2 = np.random.randint(10, 21) / 10.  
image2 = ImageEnhance.Contrast(img4).enhance(random_factor2)  
image2.show()
```

> 调用了PIL的对比度变化API，类似亮度大于1 增强，小于1 削弱。
>
> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128104134414-1674915705102-25.png" alt="image-20230128104134414" style="zoom:50%;" />



##### 锐度随机

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128104643071-1674915705102-29.png" alt="image-20230128104643071" style="zoom:67%;" />

```python
#图像锐度的随机变化
import cv2
import random
import PIL
import numpy as np
img =  PIL.Image.open('E:/DeskTop/photo/dog1.jpg')
random_factor3 = np.random.randint(0, 100) / 10.  # 随机因子
image3 = ImageEnhance.Sharpness(img).enhance(random_factor3)  # 调整图像锐度
image3.show()
```

> 调用了PIL的锐度API：
>
> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128104855238-1674915705102-31.png" alt="image-20230128104855238" style="zoom:50%;" />

## 生成对抗网络 GAN（generative adversarial network）

### 初步理解

GAN包含有两个模型，一个是生成模型G（generative model），一个是判别模型D(discriminative model)。

生成模型的任务是生成看起来自然真实的、和原始数据相似的实例。

判别模型的任务是判断给定的实例看起来是自然真实的还是人为伪造的（真实实例来源于数据集，伪造实例来源于生成模型）。

模型经过交替优化训练，两种模型都能得到提升，但最终目标是得到效果理想的生成模型。

GAN的示意图如下：

> 输入一符合简单分布（如gaussian）的随机噪声到生成模型中，Generator是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。Discriminator是一个判别网络，判别一张图片是不是“真实的”。它的输入是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/v2-48a6a2a8b213f4bd52dfb694ad292f00_1440w-1674915588614-1.png" alt="img" style="zoom:67%;" />

==**优化训练**==

对于G模型以及D模型的对抗，最终希望达到的目标是==纳什平衡==:

> 对于GAN而言，情况就是生成模型 G 恢复了训练数据的分布（造出了和真实数据一模一样的样本），判别模型再也判别不出来结果，准确率为 50%，约等于乱猜。这是双方网路都得到利益最大化，不再改变自己的策略，也就是不再更新自己的权重。

GAN的目标函数如下：

>  在训练中，希望网络D使得最大概率地分对训练样本的标签，最大化log D(x)。
>
>  希望网络G最小化log(1 – D(G(z)))，即最大化D的损失。
>
>  训练过程中固定一方的参数，更新另一个网络的参数，交替迭代，使得对方的错误最大化，最终，G 生成更加真实的样本。



![img](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/v2-64263acb7eeb012f7fa7e80446d4dac3_1440w-1674915588614-3.png)



## 项目实现

> 我基于paddle框架做了用GAN生成手写数字数据：
>
> 项目地址如下：
>
> [自动生成手写数字 经典GAN（paddle2.1）_副本 - 飞桨AI Studio (baidu.com)](https://aistudio.baidu.com/aistudio/projectdetail/5421751?contributionType=1)

判别模型discriminative model的构建：

```python
class D(paddle.nn.Layer):
    def __init__(self):
        super(D, self).__init__()

        # 第一组卷积池化
        self.conv1 = paddle.nn.Conv2D(in_channels=1,
                                      out_channels=64,
                                      kernel_size=3)
        self.bn1 = paddle.nn.BatchNorm(num_channels=64,act='relu')
        self.pool1 = paddle.nn.MaxPool2D(kernel_size=2,stride=2)
        # 第二组卷积池化
        self.conv2 = paddle.nn.Conv2D(in_channels=64,
                                      out_channels=128,
                                      kernel_size=3)
        self.bn2 = paddle.nn.BatchNorm(num_channels=128,act='relu')
        self.pool2 = paddle.nn.MaxPool2D(kernel_size=2,stride=2)
        # 全连接输出层
        self.fc1 = paddle.nn.Linear(in_features=128*5*5,out_features=1024)
        self.bnfc1 = paddle.nn.BatchNorm(num_channels=1024,act='relu')
        self.fc2 = paddle.nn.Linear(in_features=1024,out_features=2)

    def forward(self,img):
        y = self.conv1(img)
        y = self.bn1(y)
        y = self.pool1(y)
        y = self.conv2(y)
        y = self.bn2(y)
        y = self.pool2(y)
        y = y.reshape((-1,128*5*5))
        y = self.fc1(y)
        y = self.bnfc1(y)
        y = self.fc2(y)
        m = paddle.nn.Softmax()
        y = m(y)
        return y
```

生成模型G（generative model）的构建：

```python
class G(paddle.nn.Layer):
    def __init__(self):
        super(G, self).__init__()
        # 第一组全连接和BN层
        self.fc1 = paddle.nn.Linear(in_features=100,out_features=1024)
        self.bn1 = paddle.nn.BatchNorm(num_channels=1024,act='tanh')
        # 第二组全连接和BN层
        self.fc2 = paddle.nn.Linear(in_features=1024,out_features=128*7*7)
        self.bn2 = paddle.nn.BatchNorm(num_channels=128*7*7,act='tanh')
        # 第一组卷积运算（卷积前进行上采样，以扩大特征图）
        # 注：此处使用转置卷积的效果似乎并不如上采样后直接用卷积，转置卷积生成的图片噪点有点多
        self.conv1 = paddle.nn.Conv2D(in_channels=128,
                                      out_channels=64,
                                      kernel_size=5,
                                      padding=2)
        self.bn3 = paddle.nn.BatchNorm(num_channels=64,act='tanh')
        # 第二组卷积运算（卷积前进行上采样，以扩大特征图）
        self.conv2 = paddle.nn.Conv2D(in_channels=64,
                                      out_channels=1,
                                      kernel_size=5,
                                      padding=2
                                      )

    def forward(self,z):
        z = z.reshape([-1,100])
        y = self.fc1(z)
        y = self.bn1(y)
        y = self.fc2(y)
        y = self.bn2(y) # 此时y是一维的，128*7*7 = 6272
        y = y.reshape([-1,128,7,7])
        # 第一组卷积前进行上采样以扩大特征图
        # y = paddle.fluid.layers.image_resize(y, scale=2)  # 老版本写法
        up_sample_out = paddle.nn.Upsample(mode='BILINEAR',scale_factor=2)
        y = up_sample_out(y)
        # print(y.shape)
        y = self.conv1(y)
        y = self.bn3(y)
        # 第二组卷积前进行上采样以扩大特征图
        # y = paddle.fluid.layers.image_resize(y, scale=2)  # 老版本写法
        y = up_sample_out(y)
        y = self.conv2(y)
        # print('生成的图片的形状：',y.shape)
        m = paddle.nn.Tanh()
        y = m(y)
        return y
```

进行训练的函数;

```python
def new_train(epoch_num):
    for epoch in range(epoch_num):
        for batch_id,data in enumerate(zip(mnist_generator,z_generator)):
            # print(batch_id)
            mnist_data = data[0]
            z_data = data[1]
            # 丢弃不满整个batch_size的数据
            if len(mnist_data[0])!=BATCH_SIZE:
                continue
            '''
            判别器d通过最小化输入真实图片时判别器d的输出与真实标签ones的交叉熵损失，来优化判别器的参数，
            以增加判别器d识别真实图片real_image为真值标签ones的概率。
            '''
            # 将MNIST数据集里的图片读入real_image，将真值标签ones用数字1初始化
            real_image = mnist_data[0]
            ones = paddle.Tensor(np.ones([len(real_image),1]).astype('int64'))  # 标签全部都是1
            # 计算判别器d判断真实图片的真伪
            real_image = real_image.reshape([-1,1,28,28])
            p_real = d(real_image)
            # 计算判别真图片为真的损失
            real_cost = loss_fn(p_real,ones)
            # 反向传播更新判别器d的参数
            real_d_optimizer.clear_grad()
            real_cost.backward()
            real_d_optimizer.step()
            '''
            判别器d通过最小化输入生成器g生成的假图片g(z)时判别器的输出与假值标签zeros的交叉熵损失，
            来优化判别器d的参数，以增加判别器d识别生成器g生成的假图片g(z)为假值标签zeros的概率。
            '''
            # 创建高斯分布的噪声z，将假值的标签初始化为0
            z = z_data[0]
            zeros = paddle.Tensor(np.zeros([len(z),1]).astype('int64'))
            # 生成器用输入的高斯噪声z生成假图片,判别器d判断生成器g生成的假图片的概率
            p_fake = d(g(z))
            # 计算判别生成器g生成的假图片为假的损失
            fake_cost = loss_fn(p_fake,zeros)
            # 反向传播更新判别器d的参数
            fake_d_optimizer.clear_grad()
            fake_cost.backward()
            fake_d_optimizer.step()
            '''
            生成器g通过最小化判别器d判别生成器生成的假图片g(z)为真的概率d(fake)与真值标签ones的交叉熵损失，
            来优化生成器g的参数，以增加生成器g使判别器d判别其生成的假图片g(z)为真值标签ones的概率。
            '''
            # 计算判别器d判断生成器g生成的假图片的概率
            fake = g(z)
            p_confused = d(fake)
            # 使用判别器d判断生成器g生成的假图片的概率与真值ones的交叉熵计算损失
            ones = paddle.Tensor(np.ones([len(p_confused),1]).astype('int64'))  # 标签全部都是1
            g_cost = loss_fn(p_confused,ones)
            g_optimizer.clear_grad()
            g_cost.backward()
            g_optimizer.step()
            # 打印输出
            if(batch_id%400 == 0 and batch_id != 0):
                print('epoch =',epoch,',batch =',batch_id,',real_d_loss =',real_cost.numpy(),',fake_d_loss =',fake_cost.numpy(),'g_loss =',g_cost.numpy())
                show_image_grid(fake.numpy(),BATCH_SIZE,epoch)
```

我进行了50个epoch 的训练，生成的手写数字效果是很理想的，且D_loss趋近0.5，G_loss趋近1，符合目标预期。

> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128171947902-1674915588614-5.png" alt="image-20230128171947902" style="zoom:50%;" />
>
> <img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230128172029028-1674915588614-7.png" alt="image-20230128172029028" style="zoom:50%;" />







