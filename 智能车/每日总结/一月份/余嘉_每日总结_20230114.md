# 学习总结

## 计算流程图

把神经网络中的计算进行拆解，以流程的形式表示：

如下面的计算式子：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114102059754-1673705183127-1.png" alt="image-20230114102059754" style="zoom:50%;" />

可以拆解为：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114102122231-1673705183127-3.png" alt="image-20230114102122231" style="zoom:50%;" />

流程图就表示为：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114102147658-1673705183127-5.png" alt="image-20230114102147658" style="zoom:50%;" />

具体应用能够直观的表现输入数据的处理流程，当然还会用于导数的传递计算。

---



## 向量化

从向量的角度来计算，速度会快于使用列表数组的形式。网络层次扩大之后，为确保快速性，向量化是必要的。实际的时间差可能到达上百倍，应当避免for循环的出现。

例如在逻辑回归问题中，需要按照数据集列出多组表达式：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114160236916-1673705183127-7.png" alt="image-20230114160236916" style="zoom:50%;" />

向量化之后只需要一行代码就能完成这个任务：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114160348959-1673705183127-9.png" alt="image-20230114160348959" style="zoom:50%;" />

```python
Z = np.dot(w,x) + b
```

---



## np的广播机制

在向量计算中自动补齐形状不一致的矩阵，例如：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114162758838-1673705183127-11.png" alt="image-20230114162758838" style="zoom:50%;" />

将实数补齐成[1 * 4]的向量，然后正常进行运算。

---



## numpy中避免BUG的写法

```python
a = np.random.randn(5)
## 输出a.shape = (5,) ,这只是秩为1的矩阵，可能存在广播,以后的运算也可能有问题

a = np.random.randn(5,1)
## 这样定义明确输出的a.shape = (5,1) ,合理。
```



## 神经网络的表示

左侧**输入层**，中间数值无法直接取得的层级称为**隐藏层**，右侧**输出层**。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114202110038-1673705222465-19.png" alt="image-20230114202110038" style="zoom:33%;" />

在层级数量的计算上是不考虑输入层的，如上图是一个二层的神经网络。

另外在隐藏层以及输出层上都包含[w,b]的权重参数，其维度要视网络情况判断。例如上图网络的隐藏层的w维度应该是[4,3],包含四个神经元三个输入。



## 神经网络输出计算

下图是神经网络中（以逻辑回归为例），神经元包含两个部分，左侧是权重等参数的计算，右侧是引入激活函数（此处sigmoid）。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114204241852-1673705222465-21.png" alt="image-20230114204241852" style="zoom:50%;" />

基于此，隐藏层的计算式可以轻易列出：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114204617107-1673705222465-23.png" alt="image-20230114204617107" style="zoom:33%;" />

然后进行向量化，把所有式子放进矩阵里。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114205201024-1673705222465-25.png" alt="image-20230114205201024" style="zoom:50%;" />

这样就能够一次性进行整个数据集的计算。



## 激活函数

**已经提到过的sigmoid：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114215020739-1673705222465-27.png" alt="image-20230114215020739" style="zoom: 50%;" />

**tanh激活函数：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114215055652-1673705222465-29.png" alt="image-20230114215055652" style="zoom:50%;" />

**RELU激活函数：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230114215200902-1673705222465-31.png" alt="image-20230114215200902" style="zoom:50%;" />

==关于激活函数选择以及特性的问题：==

**选择激活函数的经验论：**

> sigmoid在二分问题才会考虑使用，大部分情况都不会使用。
>
> tanh具有更优越于sigmoid的效果
>
> RELU是绝大多数情况的默认激活函数，原因在于多数输入下导数值都是较大于零的数，学习效率更高。

但是sigmoid以及tanh这两激活函数都存在一个缺点：

> 当输入值远离零点（过大或者过小），函数的导数非常小。（我的印象中这个问题称之为梯度消失），这也会影响在梯度下降法使用的效果。



# 进度总结

今天的学习进度比较理想，已经完成了神经网络基础部分的学习，第二阶段的浅层神经网络也已经学习完了一半的内容。暂时还是在学习比较基础的内容，没有遇到很大的阻碍。现在是比预期进度要快了不少，估计可以提前一两天完成吴恩达DL学习的任务。









