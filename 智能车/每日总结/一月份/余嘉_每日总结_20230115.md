# 学习总结

关于使用非线性激活函数的理解：

> 采取线性激活函数的话，无论有多大的网络，其计算都是线性组合，庞大我网络也就没有意义。

## 神经网络梯度下降法

正向传递得出loss，反向传播梯度来更新权重。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230115172349077-1673793433379-1.png" alt="image-20230115172349077" style="zoom:50%;" />

具体是怎么实现的呢：

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/f529251aa7d3373f368f18d851a016b-1673793433380-3.jpg" alt="f529251aa7d3373f368f18d851a016b" style="zoom: 25%;" />

这里随意构建了一个神经网络，左侧是正向传递数据得到Loss。右侧则是通过反方向传递梯度来更新权重参数。

> 不难理解，梯度的反向传递实际的原理是链式法则，本身整个神经网络的传递就是函数关系。
>
> 还有一种理解，loss这一误差借助梯度的反向传递也分配到了各个神经元上，每个神经元都存在一定的误差值，梯度下降法的迭代就能够降低误差

如果loss存在理想的最小值，那么在迭代的过程中，梯度会越来越小，权重参数最后没什么变化了，迭代也就完成。



## 随机初始化

在神经网络中需要初始化一个参数，以此为起点进行迭代获得最佳的参数。

> 不能让初始化的值为零，各个[w,b]取零正向传递无法进行，结果也没有意义。

经验论：

>通常考虑是把初始值设为一个较小的数（靠近零），一般是0.01，原因不难理解：例如tanh激活函数，在零附近有着较大的梯度，可以让迭代过程更快的进行。

## 计算机视觉简介

应用广泛包括图像处理，目标检测，风格迁移等

此类问题通常输入数据量十分庞大，例如1000 * 1000的图像，输入数据规模是3百万。

> 常规网络无法处理如此庞大的数据量，所以视觉上常用卷积神经网络。

---

## 卷积运算

卷积运算以下图为例： 3 * 3 的卷积核在原图像上遍历，每一个位置上都是对应元素相乘并作和，得到的结果作为卷积运算结果矩阵中的一个元素。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230115203043159-1673793460500-7.png" alt="image-20230115203043159" style="zoom:67%;" />

以边缘检测为例：

下图所示的卷积核是多用于检测垂直边缘的。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230115203528650-1673793460500-9.png" alt="image-20230115203528650" style="zoom: 33%;" />

>这个基于卷积的边缘检测过程不难理解，如此形式的卷积核可以很好的把水平方向上的灰度差异表现出来，也就实现了垂直的边缘检测。



更进一步的，不同卷积核所导致的检测效果也是不一样的。如果将卷积核的具体数值全部作为参数，由神经网络学习而获得这个理想的参数，可能会达到出色的效果。

![image-20230115204546826](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230115204546826-1673793460500-11.png)

---

## Padding

>首先：直接进行卷积运算存在两个问题：
>
>1.图像每经过一次卷积运算，尺寸就会缩小（具体是 **L原图-L卷积+1**）。
>
>2.在边缘的信息都丢失了

padding 可以解决这个问题，在卷积运算之前，给原图像扩宽若干行列（拓宽的部分通常设置为零），这个扩充的范围取决于原图尺寸以及卷积核尺寸，使得卷积运算结果的shape保持不变。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230115211236065-1673793460500-13.png" alt="image-20230115211236065" style="zoom:33%;" />

所以：卷积可以分为两类：

第一类是“Valid”卷积：直接进行卷积，图像尺寸变小

第二类是"Same"卷积：padding之后再卷积，图像尺寸不变

## 卷积步长

>概念本身不难理解：卷积核每次移动的距离。

需要引入的是如何计算输出shape，或者如何求我们需要padding多少：

卷积运算结果的shape计算公式如下：
$$
\frac{length(原图像) + 2*length(padding) - length(卷积核)}{卷积步长}+1
$$

# 进度总结

由于专业还有个实验没有验收，早上请假去答辩实验了。今天还是继续看吴恩达的深度学习课程，把浅层神经网络的部分看完了，也做好了笔记总结。卷积神经网络的部分学习了一半，进度还算可观。总体学习效率还可以，线代的推导稍微多花了点时间，明天争取提前结束吴恩达的学习任务。

















