# 学习总结

## 三维卷积

三维卷积是各个通道的对应位置元素相乘，然后所有通道的所有乘积作和，结果填入单层卷积结果。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116090248493-1673880788168-1.png" alt="image-20230116090248493" style="zoom:50%;" />

>应用上：可以根据需求，针对性的设计某一通道的卷积核，实现对特定颜色通道的特征提取。

如果使用多个三维卷积核，输出的结果层数也会有变化。

>输出结果的层数等于使用的三维卷积核数

![image-20230116091117293](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116091117293-1673880788168-3.png)

---

## 池化层pooling

池化层是神经网络中的静态属性，其参数都是设定好的，无需学习。

==**最大池化：**==

过滤器在原图像上按步长移动，记录每一部分的最大值。

>这一最大值考虑包含了特征信息，所以通过pooling来获取保留特征信息

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116104602355-1673880788168-5.png" alt="image-20230116104602355" style="zoom:25%;" />

输入以及输出的层数是一致的，输入三维则输出也是三维。

> 参数的选择上通常过滤器以及步长都选择为2

==**平均池化**==

记录平均值

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116105149518-1673880788168-7.png" alt="image-20230116105149518" style="zoom:25%;" />

## 经典的深层卷积神经网络

LeNet 5

![image-20230116135253360](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116135253360-1673880804602-13.png)

> 主要的框架是多组【卷积层加上池化层】的组合，最后全连接层

## 残差网络

输入的数据直接连接到深层网络。

![image-20230116143926071](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116143926071-1673880804602-15.png)

下面式子是深度网络中加入输入的表达

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116152902397-1673880804602-17.png" alt="image-20230116152902397" style="zoom:33%;" />

ResNet网络是多个残差网络构成的：5个残差块连接在一起

![image-20230116171309786](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116171309786-1673880804602-19.png)

一般上判断随着网络层数越来越深，训练效果会越来越好，但是在普遍网络不是这样的，error在下降到某一最低点后就会上升。

ResNet网络则符合判断，网络层数越来越深，训练效果会越来越好。

![image-20230116171606893](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116171606893-1673880804602-21.png)

总结而言，引入残差块能够有效的提升深层网络的效果。

## 网络中的网络

>具体含义是指用 1 * 1 * k  的卷积核进行运算

如果网络是单层的，那么这样的卷积运算只有放大效用。

 若网络层数本身较大，这会起到如压缩这样的效果。

![image-20230116202439066](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116202439066-1673880804602-23.png)

如果采用了多个卷积核，那么输出的层数也是这个数量。

![image-20230116202654132](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116202654132-1673880804602-25.png)

## Inception网络

对前序网络的处理有很多种选择，不同大小的卷积核，以及池化层的选择。把各种选择的输出结果以原大小叠加在一起。

![image-20230116204510985](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116204510985-1673880804603-27.png)

下列是一个inception模块：

![image-20230116204833333](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116204833333-1673880804603-29.png)

完整的inception net 就是以多个模块组成。

![image-20230116205340081](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116205340081-1673880804603-31.png)



## 迁移学习

**只有较小的数据集时：**

> downloa的神经网络以及权重，冻结前部分的神经网络，只是对softmax部分的权重进行训练

![image-20230116211622090](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116211622090-1673880804603-33.png)



**具有较大的数据集时：**

> 考虑冻结较少部分的网络，对于其余部分：或是再训练一部分的网络，或是替换一部分网络。

![image-20230116212032155](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116212032155-1673880804603-35.png)



**具有十分庞大的数据集时：**

> 这种情况则所有的网络权重都从头训练，设置对应的softmax层

---

## 数据扩充

 **镜像反转**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116212915171-1673880804603-37.png" alt="image-20230116212915171" style="zoom:33%;" />

**随机剪裁：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116212958873-1673880804603-39.png" alt="image-20230116212958873" style="zoom:33%;" />

**色彩转换：**

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230116213204968.png" alt="image-20230116213204968" style="zoom:33%;" />

> 考虑了不同场景下的对象色彩差异，模型会更具有鲁棒性。





# 进度总结

### 早上：

学习完了卷积神经网络的剩余内容，包括三维卷积以及池化层等内容，然后把卷积神经网络部分的笔记整理好。

### 下午：

学习深度卷积神经网络的部分课程内容，这个部分感觉到了一些难度，有些运算的推导没法很好的理解，卡了挺久，花费了很多精力。

### 晚上：

学习完了深度卷积神经网络的课程内容，诸如残差等概念确实是第一次接触，一下子很难吸收，反复看了几遍才搞懂。今天的进度还算理想。

