# 二月

yolov8的框架调用后，出现yaml文件标签错误：

![image-20230218085312312](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218085312312.png)

由于yolov8的config文件是我自行添加的，可能标签没有做好定义，目前没有找到解决方案。

暂时放弃yolov8的模型框架。

我考虑先进行数据增强操作，开始打算使用8种数据增强的手段：

![image-20230218203108253](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218203108253.png)

增强数据集到1万张左右，训练集1万，测试集1千。处理手法是，先从干净的数据集中取出0.2部分作为测试集保留，另外的0.8部分取出，进行8种数据增强操作。

结果增强完成的数据集，我判断是不能使用的， 尤其是颜色空间扰动处理不太合适，对于这样的目标类型，我认为颜色是重要的特征之一。另外labels的修正也出现了问题，在上到ai studio的训练后，labels调整是错误的。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218203450961.png" alt="image-20230218203450961" style="zoom:50%;" />

暂且放下数据增强，先整个baseline测试。

经过阅读模型的性能参数，pp-yoloe+的x性能最优，我在baseline的基础上替换了模型框架：

![image-20230218204326075](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218204326075.png)

其次在 ai studio上的模型配置，我只对模型框架进行了替换，对于几个yml文件的重新配置，包括数据集位置等：

yml文件的参数，这个官方文档解释十分详细：

> [PaddleDetection/ppyolo_r50vd_dcn_1x_coco_annotation.md at release/2.5 · PaddlePaddle/PaddleDetection (github.com)](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/docs/tutorials/config_annotation/ppyolo_r50vd_dcn_1x_coco_annotation.md)

比较容易报错的是：

> 数据地址的配置文件：
>
> 如果位置就写到work，报错说找不到数据集文件。
>
> 第一个数据文件夹位置干脆就写绝对路径，一直写到根目录home。
>
> 后面都是相对路径。

![image-20230218205209713](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218205209713.png)

初步调整参数：

![image-20230218210039480](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218210039480.png)

学习率的调整：

可以参考下面的计算公式：

![image-20230218210149195](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218210149195.png)

执行训练命令后持续报我的数据集类型不合法：

大致error如下：

> ValueError: fruit-dection is not valid and cannot parse dataset type 'fruit-dection' for automaticly downloading, which only supports 'voc' , 'coco', 'wider_face', 'fruit', 'roadsign_voc' and 'mot' currently

尝试过修改数据集中文件夹名字等操作，但是不是问题所在。

最终解决方法如下：

找到work/PaddleDetection/ppdet/data/source/dataset.py：

注掉这段：

![image-20230218211259411](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218211259411.png)

做出下面修改：

![image-20230218211428806](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230218211428806.png)

结果是模型太大，导出的参数文件已经达到300+M，不能提交。

# 三月

## 三月17日

我一直在本地进行数据增强放大，此思路太局限且受空间限制。考虑在线上进行数据增强。

数据分析的重要性：

首先对官方数据集进行分析，第一是类别分布分析：

![image-20230317160525347](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230317160525347.png)

> 存在数据分类不均衡问题：未进行解决

> 考虑对数据集进行筛查，思路是运用的是预先训练好的模型，在评估中观察每张图像的评估得分，得分离群的图像考虑是存在问题的，提出进行单独处理。
>
> 但是eval并不好修改，调用很多源码，找不到评估的计算代码，今天没解决。

## 三月18日



我在原数据集的基础上，随机删除了400张标签数量最多的锥桶图像，

处理后标签的数量分布如下，整体上较为均衡。



![image-20230318134600196](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230318134600196.png)

进行了30轮次的训练，差不多是98，提升不显著。

以此为基础，我打算使用在线数据增强的手段，来提高数据上限：

![image-20230318204518172](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230318204518172.png)

提交后，效果不理想，我认为原因一是数据均衡用的是下采样，二是数据增强方法过于繁杂

![image-20230318225319694](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230318225319694.png)

看了下去年的冠军方案，我学习到了如下的几个思路：

> [第十七届全国大学生智能汽车竞赛：智慧交通组创意赛线上资格赛-冠军方案 - 飞桨AI Studio (baidu.com)](https://aistudio.baidu.com/aistudio/projectdetail/4217664)

> 1.考虑输入图像尺寸变更的性能影响
>
> 2.在`PaddleDetection/ppdet/modeling/heads/ppyoloe_head.py`中，将GIoULoss换成DIouLoss，对loss的优化，收敛速度或许会加速

打算明天尝试下进一步考虑输入图像尺寸变更的性能影响，多尺度是方法之一，而且我们的目标大多数小目标，我觉得整体输出尺寸的偏大对小目标检测或许会有不错的效果？



## 三月19号

我取出官方数据集中的每一类，以数量最多的锥桶为基准，对各个类别依照一定的比例进行数据增强，也就是过采样实现数据集均衡化的目的：

![image-20230319133113162](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230319133113162.png)

最终每一类的图像数量大约都在4千8左右，使用了翻转变换等初步的数据增强方法：

![image-20230319150602596](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230319150602596.png)

当然对于多尺度的输入，我有意的将随机尺寸扩大，或许对小目标的学习有正面的效果。

在训练15轮约5小时后，我取出最佳模型尝试进行了提交，效果较下采样是有所提升的：

但是尺寸的放大效果不明显。

![image-20230319185650114](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230319185650114.png)

## 三月23日

用五倍离线数据增强的大数据集进行了30h+的训练，结果不够突出，f1在0.9844，但是根据上一次的提升经验，先用一组大数据训练将loss收敛到一个0.9左右的较低值之后，再用轻量较小的原始数据集，减小学习率进一步收敛loss到0.8

![image-20230324171234336](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230324171234336.png)

## 三月24日

我直观的判断0.8的loss并不是最低值，在大数据得到的模型基础上我再次进行了训练，选择稍大的数据集以及进一步减小了学习率，效果不如原始数据集训练100轮，但是loss是更低的。

![image-20230324171410662](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230324171410662.png)

为此，我进行了一个实验，在训练中存在这样两组模型，A模型mAP(0.50, 11point) = 98.24%，loss大约是0.83，B模型mAP(0.50, 11point) = 97.85%，loss大约是0.8,同时提交了这两组模型，上面是B，下面是A：

![image-20230324171934144](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230324171934144.png)

关于关注loss，导致了过拟合问题，更低的loss，map和f1反而更低，我认为C3模型就是目前这套训练方法的上限了。





# 四月

## 四月5日

尝试用帧率来换取精度，首先尝试是正常的eval size640训练但是val以及test的输入尺寸都固定的进行了放大修改为800或者928。

<img src="https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230405224949165.png" alt="image-20230405224949165" style="zoom:67%;" />

paddledet的工程内核对于size的数据流没搞明白，最后导出export模型时，所有的尺寸必须全部都设置为在训练时的640。



其二、我将训练输入也进行的固定放大【928】，在模型导出时，val以及test的输入大小一组设置为【800】，一组设置为【928】，其结果并不理想，对于大尺寸的输入训练，结合训练过程中大尺寸的对象召回率相比于其他尺寸总是偏低，我判断较大的输入导致了大尺寸的特征提取效果不佳，而且在导出时，设置小一些的val，test输入大小反而获得了更高的结果。



其三、对于nms的测试，matrix nms效果不如multiclass，而且简单的测试了阈值，没找到最佳值。

## 四月6日

通过修改nms阈值，效果不错，提升将近万分之五，但是或许在更换模型训练参数之后，阈值也得随之出现一些变换，可以确定的是修改nms是个不错的方案。



## 四月7日

测试最佳的输出尺寸：

704：

![image-20230407134136097](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407134136097.png)

736：

![image-20230407134321067](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407134321067.png)

看趋势增大测试尺寸的输入对于小尺寸的检测增强不太明显，但是对大尺寸影响很大

672：

![image-20230407134801561](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407134801561.png)

640：

![image-20230407135453584](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407135453584.png)

608：

![image-20230407135853330](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407135853330.png)

576：

![image-20230407142331492](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407142331492.png)



就结果而言是反直觉的，在数据分析中，数据大尺寸的占比极小，大部分是小尺寸以及中尺寸，所以考虑放大输入尺寸应该是一个不错的做法，但是效果不尽人意。我认为原本图像的尺寸是240*320的，640的输入已经是放大操作了，不如从在小一点的尺寸入手，就结果而言576的输入效果却更不错。

奇怪，阈值设置为0.9，apar值全面提升了？

![image-20230407143130771](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407143130771.png)

多次测试0.8,0.9左右的值都较原来的0.7阈值有一定的精度提升，这很离谱。

提交试试，果然有问题：![image-20230407144643985](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407144643985.png)

为什么会这样？不知道原因。

但是576的尺寸输入效果确实是最佳的，阈值设置为和之前找到的0.6一致，最终在这次尝试中突破了0.99

![image-20230407150932878](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230407150932878.png)

合并数据集用同样的参数训练80轮，看看结果：

## 4月8日

把优化器修改为adam，训练到200e，提交之后成绩依旧是之前的最高精度，也就是说后一阶段的训练没有有效的loss收敛。



## 4月9日

极具的喜悦！在线的推理评估器我写出来了！虽然测试集较小，没办法复刻官方的评估系统，不过这是一个很好的调整参数以及测试模型的机会，以后可以在线测试评估之后，心里有数了再提交。



简单来说，思路就是把线上提交进行评估的流程在我们的项目里做一个丐版，把验证集的检测框数据以及类别等从xml中提取出来，按照predict.py的格式输出到一个json文件中，（predict.py中也会输出一个json），我估计官方也是拿这个json来进行评估的。我再编写了一个脚本读取两个json中的检测框数据，嵌套循环计算iou，再进一步计算出每个类别的F1等评估参数:

第一个脚本，遍历验证集xml，输出json

```python
import os
import xml.etree.ElementTree as ET
import json

# 指定VOC数据集的根目录和txt文件路径
root_dir = '/home/aistudio/data/DatasetVocSASU_ForIcarM2023/DatasetVocSASU_ForIcarM2023'
txt_file = 'data.txt'

# 读取txt文件中的图像路径
with open(txt_file, 'r') as f:
    img_paths = [line.strip() for line in f.readlines()]

annotationsx = {"result": []}
# 遍历每个图像文件，获取标注信息
for img_path in img_paths:
    # 获取图像文件名和路径
    img_filename = os.path.splitext(os.path.basename(img_path))[0]
    # img_file = os.path.join(root_dir, 'JPEGImages', img_path)

    # 获取标注文件路径
    ann_file = os.path.join(root_dir, 'Annotations', img_filename + '.xml')
    

    # 解析标注文件
    tree = ET.parse(ann_file)
    root = tree.getroot()

    # 遍历每个标注框
    for obj in root.findall('object'):
        obj_name = obj.find('name').text
        bbox = obj.find('bndbox')
        x1 = int(bbox.find('xmin').text)
        y1 = int(bbox.find('ymin').text)
        x2 = int(bbox.find('xmax').text)
        y2 = int(bbox.find('ymax').text)
        x = (x1 + x2)/2
        y = (y1 + y2)/2
        width = x2 - x1
        height = y2 - y1

        # 将标注信息添加到字典中
        ann_dict = {
            'image_id': int(img_filename), 
            'type': obj_name,
            'x': x1,
            'y': y1,
            'width': width,
            'height': height
        }
        annotationsx["result"].append(ann_dict)

with open("result_xml.json", 'w') as ft:
    json.dump(annotationsx, ft) 

```

第二个脚本：读取两个json文件，计算iou，F1。

```python
import json
from collections import defaultdict
from pathlib import Path

def calculate_iou(bbox1, bbox2):
    """计算两个边界框之间的IoU"""
    x1, y1, w1, h1 = bbox1
    x2, y2, w2, h2 = bbox2

    # 计算交集
    x_left = max(x1, x2)
    y_top = max(y1, y2)
    x_right = min(x1 + w1, x2 + w2)
    y_bottom = min(y1 + h1, y2 + h2)
    intersection = max(0, x_right - x_left) * max(0, y_bottom - y_top)

    # 计算并集
    union = w1 * h1 + w2 * h2 - intersection

    # 计算IoU
    iou = intersection / union
    return iou

# 读取验证集标注和推理结果的JSON文件
with open('result_xml.json', 'r') as f:
    validation_annotations = json.load(f)
    # print(validation_annotations)

with open('result.json', 'r') as f:
    inference_results = json.load(f)
    # print(inference_results)

# 创建字典来存储每个类别的TP、FP和FN
tp_fp_fn = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})

# 遍历推理结果的每个检测框
for detection in inference_results['result']:
    filename = Path(str(detection['image_id'])).stem
    category = detection['type']
    x = detection['x']
    y = detection['y']
    w = detection['width']
    h = detection['height']
    tp_fp_fn[category]['fp'] += 1  # 增加FP计数
    # print((filename,category,x,y,w,h))


# 遍历验证集的每个标注框
for annotation in validation_annotations['result']:
    filename = Path(str(annotation['image_id'])).stem
    category = annotation['type']
    x = annotation['x']
    y = annotation['y']
    w = annotation['width']
    h = annotation['height']
    tp_fp_fn[category]['fn'] += 1  # 增加FN计数
    # print((filename,category,x,y,w,h))
# print(tp_fp_fn[category])

# 遍历推理结果的每个检测框，并计算IoU
iou_threshold = 0.5
for detection in inference_results['result']:
    filename = Path(str(detection['image_id'])).stem
    category = detection['type']
    x1 = detection['x']
    y1 = detection['y']
    w1 = detection['width']
    h1 = detection['height']
    bbox1 = (x1,y1,w1,h1)
    max_iou = 0
    max_iou_annotation = None

    # 找到与检测框IoU最大的标注框
    for annotation in validation_annotations['result']:
        x2 = annotation['x']
        y2 = annotation['y']
        w2 = annotation['width']
        h2 = annotation['height']
        bbox2 = (x2,y2,w2,h2)
        #筛选种类以及图像是否一致
        if annotation['type'] == category and Path(str(annotation['image_id'])).stem == filename:
            iou = calculate_iou(bbox1,bbox2) 

            # 如果IoU大于某个阈值，并且检测框的类别与标注框的类别相同，则增加TP计数
            if iou > iou_threshold :
                tp_fp_fn[category]['tp'] += 1
                tp_fp_fn[category]['fp'] -= 1
                tp_fp_fn[category]['fn'] -= 1

# 打印每个类别的TP、FP和FN计数
sum_f1 = 0
for category, counts in tp_fp_fn.items():
    tp = counts['tp']
    fp = counts['fp']
    fn = counts['fn']
    precision = tp / (tp + fp) if tp + fp > 0 else 0
    recall = tp / (tp + fn) if tp + fn > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0
    sum_f1 = f1_score + sum_f1
    print(f"{category}: TP={tp}, FP={fp}, FN={fn}, Precision={precision:.6f}, Recall={recall:.6f}, F1 score={f1_score:.6f}")
avg_f1 = sum_f1/8
print("avg_F1 = {}".format(avg_f1))


```



号码校正：

1-bump

2-granary

3-crosswalk

4-cone

5-bridge

6-pig

7-tractor

8-corn

下面是漫长的调参：

[0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]

![image-20230409215522744](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409215522744.png)

[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]

![image-20230409215613292](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409215613292.png)

[0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55]

![image-20230409215634127](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409215634127.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]

![image-20230409215700408](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409215700408.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.5, 0.6]

![image-20230409215720905](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409215720905.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.6]

![image-20230409215829598](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409215829598.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.3, 0.6]

![image-20230409220050589](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409220050589.png)

[0.6, 0.6, 0.5, 0.6, 0.6, 0.6, 0.4, 0.6]

![image-20230409220257890](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409220257890.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.4]

![image-20230409221521572](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409221521572.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.5]

![image-20230409221648533](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409221648533.png)

==[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.7]==

![image-20230409221749417](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409221749417.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.65]

![image-20230409222234204](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409222234204.png)

[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.75]

![image-20230409222502347](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230409222502347.png)

心态崩了，本地调参算的F1明明更高，交到线上直接倒车0.98了

怀疑是测试集数量太低了，没什么参考效果



## 4月10日

接着试试阈值吧。

一号位阈值两个方向修改0.6,0.4都是下降

7号位阈值0.6下降，0.45也是下降

纯纯搁着瞎蒙乱猜

焯了

8号位0.37下降，0.6下降。

等待我们自己做的验证集出来在试试吧



## 4月11日



测试集做好了，鼠鼠手动标注的1900张图，怨种又来测阈值了：

![image-20230411143447893](https://yoga-typora-photo.oss-cn-beijing.aliyuncs.com/typora_img/image-20230411143447893.png)

把迭代自动搜索的脚本写完了，为了让其更加可控，我决定逐个类别的来搜索最优阈值（虽然还是感觉和官方分数出入蛮大的）：

对于0号位置的bump，从0.3到0.65以0.01的步距搜索后，0.64是F1最高的局部最佳阈值。max of f1 = 0.935778556234798



## 4月12日

没什么用，测量了四组本地的最优阈值，提交线上没有用，F1倒退。本地测量的阈值对线上没有任何意义，这搞个锤子。

行了，就在线上一个个尝试吧。至今没试出一个大于0.5的数。



## 4月13日

和leo交流明确两个问题：

第一、用模型完成寻迹任务，这个比赛周期不可行。

> 幸好在完成模型构建之前，进行了交流，以来paddledet构建的模型架构，有一定的算子在eb上是不具备ai加速算子的，根据限定的算子来搭建模型难度相当高，这个赛事周期不够开发，放弃这个寻迹方案。

第二、据称提供的标注数据集没有问题，我一直以来的担忧姑且算是解决。那么提升模型思路十分明确了，堆叠大量的数据集。

# 开门造车

==收集到的不错的学习文档：==

paddle的深度学习文档：

> [深度学习百科及面试资源 — PaddleEdu documentation (paddlepedia.readthedocs.io)](https://paddlepedia.readthedocs.io/en/latest/)

数据科学竞赛的学习教程：

> https://github.com/Mikoto10032/DeepLearning



==一些有效的trick：==

> [第十七届全国大学生智能汽车竞赛：智慧交通组创意赛线上资格赛-冠军方案 - 飞桨AI Studio (baidu.com)](https://aistudio.baidu.com/aistudio/projectdetail/4217664)

> [PP-YOLOE展示方案 - 飞桨AI Studio (baidu.com)](https://aistudio.baidu.com/aistudio/projectdetail/3665227?channelType=0&channel=0)





==完成一个机器学习任务的整体思路：==

熟悉数据集：

> 在开始构建模型之前，先对数据集进行深入了解。了解数据的特征、缺失值、异常值等信息，可以帮助你构建更好的模型。
>
> **1.如果数据集的种类框数量差别较大，一方面是均衡数据集，对loss进行修改也是不错的选择，在paddle框架下，有推荐Balanced L1 Loss作为损失函数的，能够对loss进行均衡化。**
>
> **2.对于多类别检测，非极大值抑制建议使用MultiClassSoftNMS，看大佬说多类别MultiClassSoftNMS的效果优于MultiClassNMS。**

特征工程：

> 数据的特征对于模型的性能至关重要。通过对数据进行预处理和特征工程，可以提高模型的精度和泛化能力。
>
> **1、数据增强方案：对于目标检测任务，随机的粘贴到随机背景上的随机位置上，背景可以尝试使用coco的大批量复杂数据集。**
>
> **2.用paddle自带的工具遍历返回标注框的大小分布情况，得到最符合数据的标注框尺寸，并进行预先设置。**

验证策略考虑：

> 离线进行K折交叉亦或是分组交叉验证，需要合理的设计验证集以及验证策略
>
> **1.对于Test，输入更大的图像size，牺牲帧率来换取精度**
>
> **2.对于阈值的判断，可以编写脚本遍历score的值，进行筛选，用一个较小的步距来逐步筛选阈值范围。最优置信度阈值自动寻优 ：可以考虑写一个脚本——让模型在验证集上进行推理，将所有阈值大于0.35的结果保存，并采用网格搜索的方式在0.35-0.55的范围内以0.01的步长搜索每一类的置信度阈值，计算一次平均F1值结果并保留最佳结果，再通过微调这个阈值测试出最优置信度阈值**

模型选择：

> 尝试使用不同的模型，比如线性模型、树模型、神经网络模型等，以找到最适合的模型。

模型训练：

> 使用多个模型进行集成，可以显著提高模型的性能。常用的集成方法包括堆叠、投票和平均。
>
> **1.FPN通道数的修改，缩减一定的FPN通道数量可以显著减少训练时间**
>
> **2.学习率的衰减里程碑配置在0.66的完整轮数可能是一个不错的选择，让较大的学习率多训练一段时间**
>
> **3.考虑使用测试数据增强方案，TTA, Test-Time-Augmentation。**

调参：

> 调整模型的参数可以帮助提高模型的性能。使用网格搜索或随机搜索等方法，找到最佳的参数组合。
